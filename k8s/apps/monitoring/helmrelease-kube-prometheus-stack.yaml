---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 1h
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "81.4.2"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
      interval: 1h
  install:
    crds: CreateReplace
  upgrade:
    crds: CreateReplace
  values:
    # ---- Prometheus ----
    prometheus:
      prometheusSpec:
        retention: 5d
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
        resources:
          requests:
            memory: "512Mi"
            cpu: "200m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        # Discover all ServiceMonitors, Probes, Rules without label filtering
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false

    # ---- Grafana ----
    grafana:
      enabled: true
      adminPassword: "prom-operator"
      persistence:
        enabled: true
        storageClassName: longhorn
        size: 2Gi
      ingress:
        enabled: false
      resources:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "512Mi"
          cpu: "500m"
      sidecar:
        dashboards:
          enabled: true
        datasources:
          enabled: true
      additionalDataSources:
        - name: Loki
          type: loki
          access: proxy
          url: http://loki-gateway.monitoring.svc:80
          isDefault: false
          editable: true
          jsonData:
            maxLines: 1000

    # ---- Alertmanager ----
    alertmanager:
      enabled: true
      alertmanagerSpec:
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      config:
        global:
          resolve_timeout: 5m
        receivers:
          - name: "null"
          - name: ntfy
            webhook_configs:
              - url: "http://ntfy-alertmanager.monitoring.svc:8080"
                send_resolved: true
        route:
          receiver: ntfy
          group_by:
            - alertname
            - namespace
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 4h
          routes:
            - receiver: "null"
              matchers:
                - alertname = "Watchdog"

    # ---- kube-state-metrics ----
    kube-state-metrics:
      resources:
        requests:
          memory: "64Mi"
          cpu: "50m"
        limits:
          memory: "256Mi"
          cpu: "200m"

    # ---- node-exporter ----
    # Disabled: NixOS-native node exporters run on each VM with systemd/process
    # collectors. Scrape targets and alert rules use job="nixos-node-exporter".
    nodeExporter:
      enabled: false

    # ---- Disable k3s-incompatible monitors ----
    kubeEtcd:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeProxy:
      enabled: false

    # ---- Default PrometheusRules ----
    defaultRules:
      create: true

    # ---- Custom PrometheusRules for blackbox probes ----
    additionalPrometheusRulesMap:
      blackbox-alerts:
        groups:
          - name: blackbox-exporter
            rules:
              - alert: BlackboxProbeFailed
                expr: probe_success == 0
                for: 2m
                labels:
                  severity: critical
                annotations:
                  summary: "Endpoint {{ $labels.instance }} is down"
                  description: "Blackbox probe failed for {{ $labels.instance }} for more than 2 minutes."
              - alert: BlackboxProbeSlowResponse
                expr: probe_duration_seconds > 10
                for: 5m
                labels:
                  severity: warning
                annotations:
                  summary: "Endpoint {{ $labels.instance }} is slow"
                  description: "Blackbox probe for {{ $labels.instance }} took more than 10 seconds for 5 minutes."
              - alert: BlackboxProbeHttpFailure
                expr: probe_http_status_code != 200 and probe_http_status_code != 0
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: "HTTP failure on {{ $labels.instance }}"
                  description: "HTTP status {{ $value }} on {{ $labels.instance }} for more than 2 minutes."
      external-probe-alerts:
        groups:
          - name: external-probes
            rules:
              - alert: ExternalEndpointDown
                expr: probe_success{job=~".*external-.*"} == 0
                for: 2m
                labels:
                  severity: critical
                annotations:
                  summary: "External endpoint {{ $labels.instance }} is unreachable"
                  description: "Public URL {{ $labels.instance }} has been down for 2 minutes. This indicates a DNS, Cloudflare, or Traefik issue (not just the service itself)."
              - alert: ServiceBodyAssertionFailed
                expr: probe_success{job=~".*external-.*"} == 0 and probe_http_status_code{job=~".*external-.*"} == 200
                for: 2m
                labels:
                  severity: warning
                annotations:
                  summary: "{{ $labels.instance }} returns 200 but body assertion failed"
                  description: "{{ $labels.instance }} is responding with HTTP 200 but the response body does not match expected content. The service may be functionally broken."
      vm-alerts:
        groups:
          - name: nixos-vm-health
            rules:
              - alert: VMDiskSpaceCritical
                expr: (node_filesystem_avail_bytes{job="nixos-node-exporter",fstype!="tmpfs"} / node_filesystem_size_bytes{job="nixos-node-exporter",fstype!="tmpfs"}) < 0.1
                for: 5m
                labels:
                  severity: critical
                annotations:
                  summary: "Disk space critical on {{ $labels.instance }}"
                  description: "{{ $labels.instance }} has less than 10% disk space remaining on {{ $labels.mountpoint }}."
              - alert: VMDiskSpaceWarning
                expr: (node_filesystem_avail_bytes{job="nixos-node-exporter",fstype!="tmpfs"} / node_filesystem_size_bytes{job="nixos-node-exporter",fstype!="tmpfs"}) < 0.2
                for: 10m
                labels:
                  severity: warning
                annotations:
                  summary: "Disk space low on {{ $labels.instance }}"
                  description: "{{ $labels.instance }} has less than 20% disk space remaining on {{ $labels.mountpoint }}."
              - alert: VMMemoryPressure
                expr: (node_memory_MemAvailable_bytes{job="nixos-node-exporter"} / node_memory_MemTotal_bytes{job="nixos-node-exporter"}) < 0.1
                for: 5m
                labels:
                  severity: warning
                annotations:
                  summary: "Memory pressure on {{ $labels.instance }}"
                  description: "{{ $labels.instance }} has less than 10% memory available."
              - alert: VMSystemdUnitFailed
                expr: node_systemd_unit_state{job="nixos-node-exporter",state="failed"} == 1
                for: 5m
                labels:
                  severity: warning
                annotations:
                  summary: "Systemd unit failed on {{ $labels.instance }}"
                  description: "Unit {{ $labels.name }} on {{ $labels.instance }} is in a failed state."
              - alert: PostgresConnectionsHigh
                expr: sum by (instance) (pg_stat_activity_count{job="nixos-app-exporters"}) / on (instance) pg_settings_max_connections{job="nixos-app-exporters"} > 0.8
                for: 5m
                labels:
                  severity: warning
                annotations:
                  summary: "PostgreSQL connections high on {{ $labels.instance }}"
                  description: "{{ $labels.instance }} is using more than 80% of max_connections."
      flux-alerts:
        groups:
          - name: flux
            rules:
              - alert: FluxReconciliationStalled
                expr: |
                  rate(gotk_reconcile_duration_seconds_sum{kind="GitRepository"}[10m])
                  / rate(gotk_reconcile_duration_seconds_count{kind="GitRepository"}[10m])
                  > 240
                for: 10m
                labels:
                  severity: critical
                annotations:
                  summary: "Flux {{ $labels.kind }} {{ $labels.name }} reconciliation stalled"
                  description: "Average reconcile duration for {{ $labels.kind }}/{{ $labels.name }} in {{ $labels.namespace }} exceeds 4 minutes, likely hitting the clone timeout."
              - alert: FluxReconciliationFailure
                expr: |
                  increase(gotk_reconcile_duration_seconds_count{kind=~"GitRepository|Kustomization|HelmRelease"}[30m]) == 0
                for: 30m
                labels:
                  severity: warning
                annotations:
                  summary: "Flux {{ $labels.kind }} {{ $labels.name }} has not reconciled"
                  description: "{{ $labels.kind }}/{{ $labels.name }} in {{ $labels.namespace }} has not completed a reconciliation in 30 minutes."
      opnsense-alerts:
        groups:
          - name: opnsense
            rules:
              - alert: OPNsenseGatewayDown
                expr: opnsense_gateways_status != 1
                for: 2m
                labels:
                  severity: critical
                annotations:
                  summary: "OPNsense gateway {{ $labels.name }} is down"
                  description: "Gateway {{ $labels.name }} has been offline for more than 2 minutes."
              - alert: OPNsenseHighCPU
                expr: (1 - avg(rate(node_cpu_seconds_total{job="opnsense-node-exporter",mode="idle"}[5m]))) > 0.85
                for: 10m
                labels:
                  severity: warning
                annotations:
                  summary: "OPNsense CPU usage above 85%"
                  description: "OPNsense router CPU has been above 85% for 10 minutes."
              - alert: OPNsenseWebUIDown
                expr: probe_success{instance=~".*192.168.1.1.*"} == 0
                for: 2m
                labels:
                  severity: critical
                annotations:
                  summary: "OPNsense web UI unreachable"
                  description: "OPNsense web interface at 192.168.1.1 has been down for more than 2 minutes."
      k8sgpt-alerts:
        groups:
          - name: k8sgpt
            rules:
              - alert: K8sGPTIssuesDetected
                expr: k8sgpt_number_of_results > 0
                for: 10m
                labels:
                  severity: warning
                annotations:
                  summary: "K8sGPT detected {{ $value }} cluster issues"
                  description: "K8sGPT has found issues that need attention. Inspect with: kubectl get results -n k8sgpt"
