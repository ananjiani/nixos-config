---
date: 2026-01-31
title: Cluster-wide outage after host reboot — flannel subnet.env not regenerated
severity: major
duration: ~45m
systems: [k3s, flannel, longhorn, flux]
tags: [kubernetes, networking, reboot, flannel, cni]
commit:
---

## Summary

After rebooting Proxmox hosts, the entire k3s cluster went down. No pods could start on boromir or theoden because flannel's `/run/flannel/subnet.env` file (which lives on tmpfs) was not regenerated by k3s's embedded flannel on boot. Without this file, the CNI plugin cannot assign pod IPs, cascading into: no CoreDNS, no Flux, no Longhorn, no workloads.

## Timeline

All times CST.

- **~01:00** - Proxmox hosts rebooted (boromir, theoden); k3s services restart automatically
- **~01:18** - k3s services running on all nodes, nodes report `Ready` to API server
- **01:18** - Pods on boromir and theoden stuck in `Unknown` state (stale pre-reboot pod objects)
- **~01:20** - Investigation begins via SSH to boromir
- **01:24** - All `Unknown` pods force-deleted; new pods scheduled but stuck in `ContainerCreating`
- **01:27** - `kubectl describe pod` reveals: `plugin type="flannel" failed (add): loadFlannelSubnetEnv failed: open /run/flannel/subnet.env: no such file or directory`
- **01:29** - Confirmed `/run/flannel/subnet.env` missing on boromir and theoden, present on samwise (not rebooted)
- **01:30** - k3s restarted on boromir — file still not regenerated, `flannel.1` VXLAN interface absent
- **01:33** - Manually created `/run/flannel/subnet.env` on boromir and theoden using samwise's file as reference and node Pod CIDRs from API
- **01:34** - Restarted k3s on both nodes; `flannel.1` VXLAN interfaces came up
- **01:36** - CoreDNS, Flux source-controller, kustomize-controller all running
- **01:40** - Longhorn recovering, PVCs attaching, workloads starting
- **01:45** - 68/71 pods Running, all 11 HelmReleases reconciled successfully
- **01:50** - Only codeberg-runner still broken (pre-existing config issue, not reboot-related)

## What Happened

A routine Proxmox host reboot caused boromir and theoden to restart their k3s services. The k3s processes started cleanly, the API server was responsive, and all three nodes reported `Ready`. But no pods could start on the rebooted nodes.

The initial symptoms were misleading — dozens of pods in `Unknown` state looked like a scheduling or etcd problem. Force-deleting the stale pods got new ones scheduled, but they all stuck in `ContainerCreating`. The actual error was buried in the pod events: flannel's CNI plugin couldn't find `/run/flannel/subnet.env`.

This file lives in `/run`, which is tmpfs and wiped on every reboot. k3s's embedded flannel is supposed to recreate it on startup by reading the node's Pod CIDR from the API server and writing the subnet configuration. On this boot, flannel silently failed to do this. There was no error in k3s logs about flannel initialization — it simply didn't happen. The kubelet started, found the flannel CNI config, and tried to use it, but flannel's data file wasn't there.

The fix was to manually create the file on both nodes (using the known Pod CIDRs: boromir=10.42.0.0/24, theoden=10.42.2.0/24) and restart k3s. Once the subnet.env existed, k3s created the VXLAN interfaces and the cluster recovered in a cascade: CNI working → CoreDNS up → Flux reconciles → Longhorn recovers → workloads start.

## Contributing Factors

- **k3s's embedded flannel silently fails**: No error logged when flannel doesn't initialize. The kubelet starts and tries to schedule pods anyway, leading to a wall of identical CNI errors that obscure the real issue.
- **No persistence for flannel state**: The subnet.env is the only artifact flannel needs, but it's stored exclusively in tmpfs with no backup or regeneration guarantee.
- **Stale pod objects obscure the real problem**: After a reboot, pods from before the reboot show as `Unknown`, which looks like an etcd/scheduling issue rather than a networking issue. This sent the initial investigation in the wrong direction.
- **No automated recovery or health check**: Nothing detects "flannel didn't initialize" and retries or alerts.
- **samwise not rebooted**: This was actually helpful (provided a working reference) but also masked the scope — `kubectl` worked because it ran against the API server on samwise. If all three had been rebooted simultaneously, the cluster would have been completely unmanageable.

## What I Was Wrong About

- **"Nodes are Ready means everything is fine"**: The nodes reported Ready because the kubelet was running. But Ready doesn't mean the CNI is functional. The flannel VXLAN interface didn't exist and no pods could get IPs.
- **"k3s handles reboots gracefully"**: The assumption was that k3s, being designed for edge/simple deployments, would cleanly restart all its components after a reboot. Flannel's silent initialization failure breaks this assumption.
- **"Restarting k3s will fix flannel"**: First instinct was to restart k3s on boromir. It didn't help — the same silent failure repeated. The file had to be manually seeded first.

## What Helped

- **samwise stayed up**: Having one working node meant kubectl worked, Flux controllers could be rescheduled there, and the working flannel config on samwise served as a reference template.
- **Pod CIDRs in the API**: `kubectl get nodes -o jsonpath` returned the assigned Pod CIDRs, making it straightforward to construct the correct subnet.env content.
- **Longhorn's 3-node replication**: Despite two nodes going down, no data was lost. Longhorn had replicas on samwise and volumes reattached once the CSI driver recovered.

## What Could Have Been Worse

- **All three hosts rebooted at once**: If samwise had also been rebooted, the API server would have been unreachable (all kubelets would fail CNI), and there would have been no working reference for the subnet.env content.
- **Longhorn volumes with single replica**: Any PVCs with only one replica on a rebooted node could have been stuck or lost data.
- **Remote-only access**: If this had happened while away from the LAN, the Headscale/Tailscale VPN pod would also be down, potentially locking out remote management entirely.

## Is This a Pattern?

- [ ] One-off: Correct and move on
- [x] Pattern: Revisit the approach

This reveals a systemic gap: k3s stores critical CNI state in tmpfs with no persistence or guaranteed regeneration. Any reboot risks this failure. The fix (backup/restore systemd services) addresses the immediate problem, but the underlying question is whether k3s's flannel integration is reliable enough for a multi-node cluster that gets rebooted.

## Action Items

- [x] Manually create flannel subnet.env on boromir and theoden (immediate fix)
- [x] Back up current subnet.env to `/var/lib/rancher/k3s/flannel-subnet.env` on all three nodes
- [x] Add `k3s-flannel-restore` systemd service to NixOS k3s module (runs before k3s, restores from backup)
- [x] Add `k3s-flannel-backup` systemd service to NixOS k3s module (runs after k3s, persists to disk)
- [ ] Deploy flannel fix to all three server nodes via deploy-rs
- [ ] Test the fix by rebooting one node and verifying pods schedule correctly
- [ ] File upstream k3s issue about flannel silent initialization failure
- [ ] Consider adding a cluster health check that alerts on "node Ready but no pods schedulable"

## Lessons

- **Node `Ready` does not mean the CNI works.** After a reboot, verify at least one pod can be created on each node before declaring the cluster healthy.
- **`Unknown` pods after a reboot are stale objects, not the actual problem.** Force-delete them immediately rather than investigating why they're Unknown.
- **k3s's embedded flannel can silently fail to initialize.** There is no log entry, no error, no retry. The kubelet just starts spamming CNI errors. The fix is to persist `/run/flannel/subnet.env` across reboots.
- **When debugging cascading failures, find the first domino.** The error chain was: no subnet.env → no CNI → no CoreDNS → no DNS → no Flux → no Helm upgrades → no workloads. Fixing the first link (subnet.env) resolved everything downstream.
- **Always keep at least one node up during maintenance.** Rolling reboots, not simultaneous, preserve cluster manageability.
